[{"id":1,"question":"Partitioner controls the partitioning of what data?","code":"","reference":"","choices":["final keys","final values","intermediate keys","intermediate values"],"answer":[2]},{"id":2,"question":"SQL Windowing functions are implemented in Hive using which keywords?","code":"","reference":"","choices":["UNION DISTINCT, RANK","OVER, RANK","OVER, EXCEPT","UNION DISTINCT, RANK"],"answer":[1]},{"id":3,"question":"Rather than adding a Secondary Sort to a slow Reduce job, it is Hadoop best practice to perform which optimization?","code":"","reference":"","choices":["Add a partitioned shuffle to the Map job.","Add a partitioned shuffle to the Reduce job.","Break the Reduce job into multiple, chained Reduce jobs.","Break the Reduce job into multiple, chained Map jobs."],"answer":[1]},{"id":4,"question":"Hadoop Auth enforces authentication on protected resources. Once authentication has been established, it sets what type of authenticating cookie?","code":"","reference":"","choices":["encrypted HTTP","unsigned HTTP","compressed HTTP","signed HTTP"],"answer":[3]},{"id":5,"question":"MapReduce jobs can be written in which language?","code":"","reference":"","choices":["Java or Python","SQL only","SQL or Java","Python or SQL"],"answer":[0]},{"id":6,"question":"To perform local aggregation of the intermediate outputs, MapReduce users can optionally specify which object?","code":"","reference":"","choices":["Reducer","Combiner","Mapper","Counter"],"answer":[1]},{"id":7,"question":"To verify job status, look for the value `___` in the `___`.","code":"","reference":"","choices":["SUCCEEDED; syslog","SUCCEEDED; stdout","DONE; syslog","DONE; stdout"],"answer":[1]},{"id":8,"question":"Which line of code implements a Reducer method in MapReduce 2.0?","code":"","reference":"","choices":["public void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public static void reduce(Text key, IntWritable[","public static void reduce(Text key, Iterator<IntWritable> values, Context context){…}","public void reduce(Text key, IntWritable["],"answer":[0]},{"id":9,"question":"To get the total number of mapped input records in a map job task, you should review the value of which counter?","code":"","reference":"","choices":["FileInputFormatCounter","FileSystemCounter","JobCounter","TaskCounter (NOT SURE)"],"answer":[3]},{"id":10,"question":"Hadoop Core supports which CAP capabilities?","code":"","reference":"","choices":["A, P","C, A","C, P","C, A, P"],"answer":[0]},{"id":11,"question":"What are the primary phases of a Reducer?","code":"","reference":"","choices":["combine, map, and reduce","shuffle, sort, and reduce","reduce, sort, and combine","map, sort, and combine"],"answer":[1]},{"id":12,"question":"To set up Hadoop workflow with synchronization of data between jobs that process tasks both on disk and in memory, use the `___` service, which is `___`.","code":"","reference":"","choices":["Oozie; open source","Oozie; commercial software","Zookeeper; commercial software","Zookeeper; open source"],"answer":[3]},{"id":13,"question":"For high availability, use multiple nodes of which type?","code":"","reference":"","choices":["data","name","memory","worker"],"answer":[1]},{"id":14,"question":"DataNode supports which type of drives?","code":"","reference":"","choices":["hot swappable","cold swappable","warm swappable","non-swappable"],"answer":[0]},{"id":15,"question":"Which method is used to implement Spark jobs?","code":"","reference":"","choices":["on disk of all workers","on disk of the master node","in memory of the master node","in memory of all workers"],"answer":[3]},{"id":16,"question":"In a MapReduce job, where does the map() function run?","code":"","reference":"","choices":["on the reducer nodes of the cluster","on the data nodes of the cluster (NOT SURE)","on the master node of the cluster","on every node of the cluster"],"answer":[1]},{"id":17,"question":"To reference a master file for lookups during Mapping, what type of cache should be used?","code":"","reference":"","choices":["distributed cache","local cache","partitioned cache","cluster cache"],"answer":[0]},{"id":18,"question":"Skip bad records provides an option where a certain set of bad input records can be skipped when processing what type of data?","code":"","reference":"","choices":["cache inputs","reducer inputs","intermediate values","map inputs"],"answer":[3]},{"id":19,"question":"Which command imports data to Hadoop from a MySQL database?","code":"","reference":"","choices":["spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --warehouse-dir user/hue/oozie/deployments/spark","sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","sqoop import --connect jdbc:mysql://mysql.example.com/sqoop --username sqoop --password sqoop --warehouse-dir user/hue/oozie/deployments/sqoop","spark import --connect jdbc:mysql://mysql.example.com/spark --username spark --password spark --warehouse-dir user/hue/oozie/deployments/spark"],"answer":[2]},{"id":20,"question":"In what form is Reducer output presented?","code":"","reference":"","choices":["compressed (NOT SURE)","sorted","not sorted","encrypted"],"answer":[0]},{"id":21,"question":"Which library should be used to unit test MapReduce code?","code":"","reference":"","choices":["JUnit","XUnit","MRUnit","HadoopUnit"],"answer":[2]},{"id":22,"question":"If you started the NameNode, then which kind of user must you be?","code":"","reference":"","choices":["hadoop-user","super-user","node-user","admin-user"],"answer":[1]},{"id":23,"question":"State _ between the JVMs in a MapReduce job","code":"","reference":"","choices":["can be configured to be shared","is partially shared","is shared","is not shared (https://www.lynda.com/Hadoop-tutorials/Understanding-Java-virtual-machines-JVMs/191942/369545-4.html)"],"answer":[3]},{"id":24,"question":"To create a MapReduce job, what should be coded first?","code":"","reference":"","choices":["a static job() method","a Job class and instance (NOT SURE)","a job() method","a static Job class"],"answer":[1]},{"id":25,"question":"To connect Hadoop to AWS S3, which client should you use?","code":"","reference":"","choices":["S3A","S3N","S3","the EMR S3"],"answer":[0]},{"id":26,"question":"HBase works with which type of schema enforcement?","code":"","reference":"","choices":["schema on write","no schema","external schema","schema on read"],"answer":[3]},{"id":27,"question":"HDFS file are of what type?","code":"","reference":"","choices":["read-write","read-only","write-only","append-only"],"answer":[3]},{"id":28,"question":"A distributed cache file path can originate from what location?","code":"","reference":"","choices":["hdfs or top","http","hdfs or http","hdfs"],"answer":[2]},{"id":29,"question":"Which library should you use to perform ETL-type MapReduce jobs?","code":"","reference":"","choices":["Hive","Pig","Impala","Mahout"],"answer":[1]},{"id":30,"question":"What is the output of the Reducer?","code":"`map function processes a certain key-value pair and emits a certain number of key-value pairs and the Reduce function processes values grouped by the same key and emits another set of key-value pairs as output.`","reference":"`map function processes a certain key-value pair and emits a certain number of key-value pairs and the Reduce function processes values grouped by the same key and emits another set of key-value pairs as output.`","choices":["a relational table","an update to the input file","a single, combined list","a set of <key, value> pairs"],"answer":[3]},{"id":31,"question":"To optimize a Mapper, what should you perform first?","code":"","reference":"","choices":["Override the default Partitioner.","Skip bad records.","Break up Mappers that do more than one task into multiple Mappers.","Combine Mappers that do one task into large Mappers."],"answer":[]},{"id":32,"question":"When implemented on a public cloud, with what does Hadoop processing interact?","code":"","reference":"","choices":["files in object storage","graph data in graph databases","relational data in managed RDBMS systems","JSON data in NoSQL databases"],"answer":[0]},{"id":33,"question":"In the Hadoop system, what administrative mode is used for maintenance?","code":"### Q34. In what format does RecordWriter write an output file? ### Q35. To what does the Mapper map input key/value pairs? ### Q36. Which Hive query returns the first 1,000 values? ### Q37. To implement high availability, how many instances of the master node should you configure? ### Q38. Hadoop 2.x and later implement which service as the resource coordinator? ### Q39. In MapReduce, **_** have _ ### Q40. What type of software is Hadoop Common? ### Q41. If no reduction is desired, you should set the numbers of _ tasks to zero ### Q42. MapReduce applications use which of these classes to report their statistics? ### Q43. _ is the query language, and _ is storage for NoSQL on Hadoop ### Q44. MapReduce 1.0 _ YARN ### Q45. Which type of Hadoop node executes file system namespace operations like opening, closing, and renaming files and directories? ### Q46. HQL queries produce which job types? ### Q47 Suppose you are trying to finish a Pig script that converts text in the input string to uppercase. What code is needed on line 2 below?     1 data = LOAD '/user/hue/pig/examples/data/midsummer.txt'...     2 ### Q48. In a MapReduce job, which phase runs after the Map phase completes? ### Q49. Where would you configure the size of a block in a Hadoop environment? ### Q50. Hadoop systems are **_** RDBMS systems. ### Q51. Which object can be used to distribute jars or libraries for use in MapReduce tasks? ### Q52. To view the execution details of an Impala query plan, which function would you use ? ### Q53. Which feature is used to roll back a corrupted HDFS instance to a previously known good point in time? ### Q54. Hadoop Common is written in which language? ### Q55. Which file system does Hadoop use for storage? ### Q56. What kind of storage and processing does Hadoop support? ### Q57. Hadoop Common consists of which components? ### Q58. Most Apache Hadoop committers' work is done at which commercial company? ### Q59. To get information about Reducer job runs, which object should be added? ### Q60. After changing the default block size and restarting the cluster, to which data does the new size apply? ### Q61. Which statement should you add to improve the performance of the following query?  SELECT   c.id,   c.name,   c.email_preferences.categories.surveys FROM customers c;  ### Q62. What custom object should you implement to reduce IO in MapReduce? ### Q63. You can optimize Hive queries using which method? ### Q64. If you are processing a single action on each input, what type of job should you create? ### Q65. The simplest possible MapReduce job optimization is to perform which of these actions? ### Q66. When you implement a custom Writable, you must also define which of these object?","reference":"### Q66. When you implement a custom Writable, you must also define which of these object?","choices":["data mode","safe mode","single-user mode","pseudo-distributed mode","<key, value> pairs","keys","values","<value, key> pairs","an average of keys for values","a sum of keys for values","a set of intermediate key/value pairs","a set of final key/value pairs","SELECT…WHERE value = 1000","SELECT … LIMIT 1000","SELECT TOP 1000 …","SELECT MAX 1000…","one","zero","shared","two or more (https://data-flair.training/blogs/hadoop-high-availability-tutorial)","kubernetes","JobManager","JobTracker","YARN","tasks; jobs","jobs; activities","jobs; tasks","activities; tasks","database","distributed computing framework","operating system","productivity tool","combiner","reduce","mapper","intermediate","mapper","reducer","combiner","counter","HDFS; HQL","HQL; HBase","HDFS; SQL","SQL; HBase","does not include","is the same thing as","includes","replaces","ControllerNode","DataNode","MetadataNode","NameNode","Impala","MapReduce","Spark","Pig","as (text:CHAR[]); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHARARRAY); upper_case = FOREACH data GENERATE org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHAR[]); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","as (text:CHARARRAY); upper_case = FOREACH data org.apache.pig.piggybank.evaluation.string.UPPER(TEXT);","Combiner","Reducer","Map2","Shuffle and Sort","dfs.block.size in hdfs-site.xmls","orc.write.variable.length.blocks in hive-default.xml","mapreduce.job.ubertask.maxbytes in mapred-site.xml","hdfs.block.size in hdfs-site.xml","replacements for","not used with","substitutes for","additions for","distributed cache","library manager","lookup store","registry","explain","query action","detail","query plan","partitioning","snapshot","replication","high availability","C++","C","Haskell","Java","NAS","FAT","HDFS","NFS","encrypted","verified","distributed","remote","Spark and YARN","HDFS and MapReduce","HDFS and S3","Spark and MapReduce","Cloudera","Microsoft","Google","Amazon","Reporter","IntReadable","IntWritable","Writer","all data","no data","existing data","new data","GROUP BY","FILTER","SUB-SELECT","SORT","Comparator","Mapper","Combiner","Reducer","secondary indices","summary statistics","column-based statistics","a primary key index","partition-only","map-only","reduce-only","combine-only","Add more master nodes.","Implement optimized InputSplits.","Add more DataNodes.","Implement a custom Mapper.","a sort policy","a combiner policy","a compression policy","a filter policy"],"answer":[1,4,10,13,19,23,26,29,33,39,41,44,51,57,60,64,71,72,76,81,87,90,94]}]